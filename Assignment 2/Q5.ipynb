{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"QjHK80jR15cb"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\bhara\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\bhara\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","nltk.download('punkt')\n","\n","import numpy as np\n","import pandas as pd\n","from gensim.models import word2vec\n","\n","import re # For regular expressions"]},{"cell_type":"markdown","metadata":{"id":"UTam_xXUv2Z9"},"source":["## (a) Load the dataset"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"xQsTJcH2X-9E"},"outputs":[{"name":"stdout","output_type":"stream","text":["[['upgrade', 'b', 'lasgtbwi', 'brainer', 'week', 'thanks'], ['flight', 'delayed', 'orig', 'departure', 'time', 'adhere', 'original', 'time', 'drop', 'bags', 'arrive', 'gate', 'new', 'time', 'works', 'airport', 'delays'], ['currently', 'flight', 'delayed', 'hour', 'reparked', 'kickoff', 'passenger', 'mins', 'later', 'let', 'back', 'offer', 'drinks', 'food', 'terrible', 'flown']]\n"]}],"source":["def load_data():\n","    \"\"\" Read tweets from the file.\n","        Return:\n","            list of lists (list_words), with words from each of the processed tweets\n","    \"\"\"\n","    tweets = pd.read_csv('tweets.csv', names=['text'])\n","    list_words = []\n","    ### iterate over all tweets from the dataset\n","    for i in tweets.index:\n","      ### remove non-letter.\n","      text = re.sub(\"[^a-zA-Z ]\", \"\", tweets.iloc[i].text)\n","      ### tokenize\n","      words = text.split()\n","\n","      stop_words = set(stopwords.words(\"english\"))\n","      \n","      new_words = []\n","      ### iterate over all words of a tweet\n","      for w in words:\n","        ## TODO: remove the stop words and convert a word (w) to the lower case\n","        if w.lower() not in stop_words:\n","          new_words.append(w.lower())\n","        \n","      list_words.append(new_words)\n","    return list_words\n","\n","# check a few samples of twitter corpus\n","twitter_corpus = load_data()\n","print(twitter_corpus[:3])"]},{"cell_type":"markdown","metadata":{"id":"e-ZkbmSX15ck"},"source":["## (b) Create co-occurrence matrix"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"X3B83uir15cn"},"outputs":[{"name":"stdout","output_type":"stream","text":["['aa', 'aaaaand', 'aaaahhh', 'aaaand', 'aampb', 'aawish', 'aba', 'abacus', 'abat', 'abc'] 10841\n"]}],"source":["def distinct_words(corpus):\n","    \"\"\" get a list of distinct words for the corpus.\n","        Params:\n","            corpus (list of list of strings): corpus of documents\n","        Return:\n","            corpus_words (list of strings): list of distinct words across the corpus, sorted (using python 'sorted' function)\n","            num_corpus_words (integer): number of distinct words across the corpus\n","    \"\"\"\n","    corpus_words = []\n","    num_corpus_words = -1\n","    # ------------------\n","    # TODO:\n","    for tweet in corpus:\n","        for word in tweet:\n","            if word not in corpus_words:\n","                corpus_words.append(word)\n","    \n","    num_corpus_words = len(corpus_words)\n","    corpus_words = sorted(corpus_words)\n","\n","    # ------------------\n","    return corpus_words, num_corpus_words\n","\n","words, num_words = distinct_words(twitter_corpus)\n","print(words[:10], num_words)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"8WB4cZBR15cp","scrolled":true},"outputs":[],"source":["def compute_co_occurrence_matrix(corpus, window_size=5):\n","    \"\"\" Compute co-occurrence matrix for the given corpus and window_size (default of 5).    \n","        Params:\n","            corpus (list of list of strings): corpus of documents\n","            window_size (int): size of context window\n","        Return:\n","            M (numpy matrix of shape = [number of corpus words x number of corpus words]): \n","                Co-occurence matrix of word counts. \n","                The ordering of the words in the rows/columns should be the same as the ordering of the words given by the distinct_words function.\n","            word2Ind (dict): dictionary that maps word to index (i.e. row/column number) for matrix M.\n","    \"\"\"\n","    M = np.zeros((num_words, num_words))\n","    word2Ind = {word:i for i, word in enumerate(words)}\n","    \n","    # ------------------\n","    # TODO:\n","    for tweet in corpus:\n","        for idx, word in enumerate(tweet):\n","            snippet = tweet[idx-window_size:idx+window_size]\n","            for winword in snippet:\n","                if word == winword:\n","                    continue\n","                M[word2Ind[word]][word2Ind[winword]] += 1\n","\n","    # ------------------\n","\n","    return M, word2Ind\n","\n","M, word2Ind = compute_co_occurrence_matrix(twitter_corpus)"]},{"cell_type":"markdown","metadata":{"id":"11njtWHx15cv"},"source":["## (c) SVD"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"LddeVOq615cv"},"outputs":[],"source":["# -----------------------------\n","# Run SVD\n","# Note: This may take several minutes\n","svd = np.linalg.svd(M)\n","# ------------------------------"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"data":{"text/plain":["array([-1.17753923e-03, -1.70919369e-03,  1.34122524e-05, -3.24855321e-04,\n","        4.36910487e-04, -6.25674420e-04,  1.74534635e-03,  4.69460153e-03,\n","       -2.41173383e-03, -2.67955561e-03, -1.72447623e-03,  7.37985022e-04,\n","       -7.67156010e-04,  1.45069326e-03, -1.88542498e-03,  4.61357977e-03,\n","        1.67730203e-03,  2.38072866e-03,  2.51411184e-03,  5.57206676e-03,\n","       -7.74304613e-04, -6.22149371e-03, -5.88557659e-04,  2.70487346e-03,\n","       -3.39341299e-04, -5.92755373e-03, -7.23058413e-03,  2.68102948e-03,\n","        1.04587764e-03, -2.20739522e-03,  6.23608034e-03,  4.54652242e-03,\n","       -2.30011801e-03,  8.63077415e-03,  7.68760995e-04,  2.23827307e-03,\n","       -1.46133409e-04, -2.02301050e-03, -2.56462792e-04,  8.99979917e-04,\n","        1.83894116e-03,  8.69968459e-04,  1.10336983e-02,  1.68459899e-03,\n","        1.31351208e-02, -6.41530442e-03, -2.98473395e-03,  2.04847670e-03,\n","       -9.13565806e-04, -5.32133961e-03])"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["u, s, vt = svd\n","svdvec = u[:,0:50]\n","svdvec[0]"]},{"cell_type":"markdown","metadata":{"id":"D4dOMjKj15cy"},"source":["## (d) Word2Vec"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"jRti6Rn815cy"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training Word2Vec model....\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_15524\\3237888261.py:14: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n","  model.init_sims(replace=True)\n"]}],"source":["# Creating the word2vec model and setting values for the various parameters\n","\n","# Initializing the train model. \n","num_features = 50   # Word vector dimensionality\n","min_word_count = 0  # Minimum word count. You can change it also.\n","num_workers = 4     # Number of parallel threads, can be changed\n","context = 5         # Context window size\n","downsampling = 1e-3 # (0.001) Downsample setting for frequent words, can be changed\n","# Initializing the train model\n","print(\"Training Word2Vec model....\")\n","model = word2vec.Word2Vec(twitter_corpus)\n","\n","# To make the model memory efficient\n","model.init_sims(replace=True)"]},{"cell_type":"markdown","metadata":{"id":"asqnIK1315c0"},"source":["## (d) Compare SVD word embeddings with Word2Vec"]},{"cell_type":"code","execution_count":111,"metadata":{"id":"bwGC7K0z15c3"},"outputs":[],"source":["from sklearn.metrics.pairwise import cosine_similarity\n","\n","def svd_most_similar(query_word, n=10):\n","    \"\"\" return 'n' most similar words of a query word using the SVD word embeddings similar to word2vec's most_smilar    \n","        Params:\n","            query_word (strings): a query word\n","        Return:\n","            most_similar (list of strings): the list of 'n' most similar words\n","    \"\"\"\n","    n += 1\n","    wvec = svdvec[word2Ind[query_word]].reshape([1, 50])\n","    simscores = cosine_similarity(wvec, svdvec)[0]\n","    idx = np.argpartition(simscores, -n)[-n:]\n","    idx = idx[np.argsort(simscores[idx])[::-1]]\n","    most_similar = [word for word, i in word2Ind.items() if i in idx and word != query_word]\n","\n","    return most_similar"]},{"cell_type":"markdown","metadata":{"id":"W3Fbmd6y15c4"},"source":["## SVD vs Word2Vec: \"???\""]},{"cell_type":"code","execution_count":114,"metadata":{"id":"zBzFmNpH15c5"},"outputs":[{"data":{"text/plain":["['cantplanthehoneymoonwithoutwifi',\n"," 'dangling',\n"," 'fasten',\n"," 'fufuaufufuu',\n"," 'fufuuafufufubbudfufuubbuufef',\n"," 'grants',\n"," 'ling',\n"," 'terminalthat',\n"," 'unload',\n"," 'upload']"]},"execution_count":114,"metadata":{},"output_type":"execute_result"}],"source":["svd_most_similar(\"delay\")"]},{"cell_type":"code","execution_count":115,"metadata":{"id":"BxVYovyL15c5"},"outputs":[{"data":{"text/plain":["[('delayed', 0.997496485710144),\n"," ('hour', 0.9972133636474609),\n"," ('almost', 0.9969688653945923),\n"," ('late', 0.9969677925109863),\n"," ('waiting', 0.9969348311424255),\n"," ('sitting', 0.9968117475509644),\n"," ('half', 0.9966129064559937),\n"," ('plane', 0.996375322341919),\n"," ('vegas', 0.9962120056152344),\n"," ('going', 0.9960824847221375)]"]},"execution_count":115,"metadata":{},"output_type":"execute_result"}],"source":["model.wv.most_similar(\"delay\") #this word2vec trained model on tweets"]},{"cell_type":"code","execution_count":116,"metadata":{"id":"tWWyBDGPpYcp"},"outputs":[{"data":{"text/plain":["['aaaaand',\n"," 'alistmember',\n"," 'firstmarathon',\n"," 'image',\n"," 'mar',\n"," 'midday',\n"," 'pouring',\n"," 'seniors',\n"," 'thanksdavid',\n"," 'underonaflight']"]},"execution_count":116,"metadata":{},"output_type":"execute_result"}],"source":["svd_most_similar(\"flight\")"]},{"cell_type":"code","execution_count":117,"metadata":{"id":"UcsipeIhpaHh"},"outputs":[{"data":{"text/plain":["[('hrs', 0.9984350204467773),\n"," ('plane', 0.9983459115028381),\n"," ('lax', 0.9983003735542297),\n"," ('almost', 0.9981791377067566),\n"," ('pm', 0.9981695413589478),\n"," ('vegas', 0.9981691241264343),\n"," ('connecting', 0.9981654286384583),\n"," ('late', 0.9981614351272583),\n"," ('weather', 0.9981505870819092),\n"," ('airport', 0.9981454014778137)]"]},"execution_count":117,"metadata":{},"output_type":"execute_result"}],"source":["model.wv.most_similar(\"flight\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A1X7P3q-pcEK"},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3.10.1 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.1"},"vscode":{"interpreter":{"hash":"2ca6819a7b235a9e9923d27ebe966cb7261666ab2e4abf79b2eb7e1de2b3fef6"}}},"nbformat":4,"nbformat_minor":0}
